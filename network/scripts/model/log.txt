Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.05
epochs: 60
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2022-09-12 14:50:38 ===============
=> Current Lr: 0.05
[0/300]: 1.7536
[20/300]: 0.4060
[40/300]: 0.0730
[60/300]: 0.0747
[80/300]: 0.0461
[100/300]: 0.0424
[120/300]: 0.0391
[140/300]: 0.0420
[160/300]: 0.0344
[180/300]: 0.0322
[200/300]: 0.0347
[220/300]: 0.0292
[240/300]: 0.0345
[260/300]: 0.0281
[280/300]: 0.0349
=> Training Loss: 0.3400, Evaluation Loss 0.0349

============= Epoch 1 | 2022-09-12 14:52:15 ===============
=> Current Lr: 0.05
[0/300]: 0.0263
[20/300]: 0.0288
[40/300]: 0.0397
[60/300]: 0.0302
[80/300]: 0.0295
[100/300]: 0.0292
[120/300]: 0.0231
[140/300]: 0.0370
[160/300]: 0.0265
[180/300]: 0.0196
[200/300]: 0.0292
[220/300]: 0.0283
[240/300]: 0.0301
[260/300]: 0.0235
[280/300]: 0.0283
=> Training Loss: 0.0287, Evaluation Loss 0.0274

============= Epoch 2 | 2022-09-12 14:53:51 ===============
=> Current Lr: 0.05
[0/300]: 0.0263
[20/300]: 0.0280
[40/300]: 0.0281
[60/300]: 0.0199
[80/300]: 0.0381
[100/300]: 0.0301
[120/300]: 0.0243
[140/300]: 0.0303
[160/300]: 0.0220
[180/300]: 0.0180
[200/300]: 0.0264
[220/300]: 0.0240
[240/300]: 0.0285
[260/300]: 0.0232
[280/300]: 0.0247
=> Training Loss: 0.0276, Evaluation Loss 0.0318

============= Epoch 3 | 2022-09-12 14:55:33 ===============
=> Current Lr: 0.05
[0/300]: 0.0346
[20/300]: 0.0397
[40/300]: 0.0261
[60/300]: 0.0193
[80/300]: 0.0248
[100/300]: 0.0273
[120/300]: 0.0268
[140/300]: 0.0240
[160/300]: 0.0199
[180/300]: 0.0318
[200/300]: 0.0177
[220/300]: 0.0358
[240/300]: 0.0317
[260/300]: 0.0200
[280/300]: 0.0274
=> Training Loss: 0.0269, Evaluation Loss 0.0286

============= Epoch 4 | 2022-09-12 14:57:18 ===============
=> Current Lr: 0.05
[0/300]: 0.0256
[20/300]: 0.0359
[40/300]: 0.0411
[60/300]: 0.0326
[80/300]: 0.0253
[100/300]: 0.0305
[120/300]: 0.0316
[140/300]: 0.0268
[160/300]: 0.0253
[180/300]: 0.0300
[200/300]: 0.0277
[220/300]: 0.0322
[240/300]: 0.0248
[260/300]: 0.0305
[280/300]: 0.0326
=> Training Loss: 0.0277, Evaluation Loss 0.0310

============= Epoch 5 | 2022-09-12 14:59:04 ===============
=> Current Lr: 0.025
[0/300]: 0.0235
[20/300]: 0.0182
[40/300]: 0.0213
[60/300]: 0.0167
[80/300]: 0.0171
[100/300]: 0.0222
[120/300]: 0.0214
[140/300]: 0.0276
[160/300]: 0.0245
[180/300]: 0.0172
[200/300]: 0.0183
[220/300]: 0.0188
[240/300]: 0.0212
[260/300]: 0.0217
[280/300]: 0.0329
=> Training Loss: 0.0232, Evaluation Loss 0.0285

============= Epoch 6 | 2022-09-12 15:00:52 ===============
=> Current Lr: 0.025
[0/300]: 0.0204
[20/300]: 0.0267
[40/300]: 0.0228
[60/300]: 0.0167
[80/300]: 0.0249
[100/300]: 0.0310
[120/300]: 0.0247
[140/300]: 0.0248
[160/300]: 0.0232
[180/300]: 0.0220
[200/300]: 0.0231
[220/300]: 0.0216
[240/300]: 0.0154
[260/300]: 0.0206
[280/300]: 0.0225
=> Training Loss: 0.0228, Evaluation Loss 0.0250

============= Epoch 7 | 2022-09-12 15:02:40 ===============
=> Current Lr: 0.025
[0/300]: 0.0242
[20/300]: 0.0286
[40/300]: 0.0238
[60/300]: 0.0176
[80/300]: 0.0303
[100/300]: 0.0230
[120/300]: 0.0258
[140/300]: 0.0190
[160/300]: 0.0256
[180/300]: 0.0273
[200/300]: 0.0240
[220/300]: 0.0177
[240/300]: 0.0125
[260/300]: 0.0242
[280/300]: 0.0188
=> Training Loss: 0.0233, Evaluation Loss 0.0316

============= Epoch 8 | 2022-09-12 15:04:29 ===============
=> Current Lr: 0.025
[0/300]: 0.0229
[20/300]: 0.0275
[40/300]: 0.0224
[60/300]: 0.0252
[80/300]: 0.0169
[100/300]: 0.0332
[120/300]: 0.0232
[140/300]: 0.0289
[160/300]: 0.0202
[180/300]: 0.0235
[200/300]: 0.0185
[220/300]: 0.0244
[240/300]: 0.0171
[260/300]: 0.0219
[280/300]: 0.0171
=> Training Loss: 0.0229, Evaluation Loss 0.0211

============= Epoch 9 | 2022-09-12 15:06:18 ===============
=> Current Lr: 0.025
[0/300]: 0.0267
[20/300]: 0.0204
[40/300]: 0.0275
[60/300]: 0.0238
[80/300]: 0.0268
[100/300]: 0.0245
[120/300]: 0.0207
[140/300]: 0.0171
[160/300]: 0.0271
[180/300]: 0.0304
[200/300]: 0.0246
[220/300]: 0.0191
[240/300]: 0.0236
[260/300]: 0.0215
[280/300]: 0.0288
=> Training Loss: 0.0229, Evaluation Loss 0.0239

============= Epoch 10 | 2022-09-12 15:08:06 ==============
=> Current Lr: 0.0125
[0/300]: 0.0189
[20/300]: 0.0168
[40/300]: 0.0251
[60/300]: 0.0213
[80/300]: 0.0135
[100/300]: 0.0245
[120/300]: 0.0241
[140/300]: 0.0215
[160/300]: 0.0186
[180/300]: 0.0171
[200/300]: 0.0198
[220/300]: 0.0240
[240/300]: 0.0190
[260/300]: 0.0214
[280/300]: 0.0153
=> Training Loss: 0.0206, Evaluation Loss 0.0269

============= Epoch 11 | 2022-09-12 15:09:55 ==============
=> Current Lr: 0.0125
[0/300]: 0.0258
[20/300]: 0.0183
[40/300]: 0.0263
[60/300]: 0.0179
[80/300]: 0.0241
[100/300]: 0.0223
[120/300]: 0.0161
[140/300]: 0.0208
[160/300]: 0.0148
[180/300]: 0.0176
[200/300]: 0.0177
[220/300]: 0.0220
[240/300]: 0.0232
[260/300]: 0.0155
[280/300]: 0.0130
=> Training Loss: 0.0203, Evaluation Loss 0.0192

============= Epoch 12 | 2022-09-12 15:11:45 ==============
=> Current Lr: 0.0125
[0/300]: 0.0192
[20/300]: 0.0275
[40/300]: 0.0139
[60/300]: 0.0157
[80/300]: 0.0170
[100/300]: 0.0172
[120/300]: 0.0190
[140/300]: 0.0141
[160/300]: 0.0243
[180/300]: 0.0236
[200/300]: 0.0158
[220/300]: 0.0272
[240/300]: 0.0171
[260/300]: 0.0161
[280/300]: 0.0248
=> Training Loss: 0.0199, Evaluation Loss 0.0188

============= Epoch 13 | 2022-09-12 15:13:36 ==============
=> Current Lr: 0.0125
[0/300]: 0.0184
[20/300]: 0.0151
[40/300]: 0.0219
[60/300]: 0.0245
[80/300]: 0.0221
[100/300]: 0.0210
[120/300]: 0.0176
[140/300]: 0.0183
[160/300]: 0.0223
[180/300]: 0.0182
[200/300]: 0.0218
[220/300]: 0.0164
[240/300]: 0.0160
[260/300]: 0.0170
[280/300]: 0.0143
=> Training Loss: 0.0200, Evaluation Loss 0.0216

============= Epoch 14 | 2022-09-12 15:15:25 ==============
=> Current Lr: 0.0125
[0/300]: 0.0142
[20/300]: 0.0220
[40/300]: 0.0215
[60/300]: 0.0162
[80/300]: 0.0184
[100/300]: 0.0240
[120/300]: 0.0176
[140/300]: 0.0155
[160/300]: 0.0208
[180/300]: 0.0177
[200/300]: 0.0169
[220/300]: 0.0203
[240/300]: 0.0173
[260/300]: 0.0194
[280/300]: 0.0225
=> Training Loss: 0.0197, Evaluation Loss 0.0235

============= Epoch 15 | 2022-09-12 15:17:14 ==============
=> Current Lr: 0.00625
[0/300]: 0.0170
[20/300]: 0.0267
[40/300]: 0.0149
[60/300]: 0.0190
[80/300]: 0.0154
[100/300]: 0.0168
[120/300]: 0.0163
[140/300]: 0.0173
[160/300]: 0.0128
[180/300]: 0.0186
[200/300]: 0.0125
[220/300]: 0.0194
[240/300]: 0.0157
[260/300]: 0.0210
[280/300]: 0.0173
=> Training Loss: 0.0176, Evaluation Loss 0.0173

============= Epoch 16 | 2022-09-12 15:19:03 ==============
=> Current Lr: 0.00625
[0/300]: 0.0187
[20/300]: 0.0185
[40/300]: 0.0153
[60/300]: 0.0169
[80/300]: 0.0168
[100/300]: 0.0142
[120/300]: 0.0180
[140/300]: 0.0183
[160/300]: 0.0176
[180/300]: 0.0170
[200/300]: 0.0126
[220/300]: 0.0191
[240/300]: 0.0171
[260/300]: 0.0135
[280/300]: 0.0128
=> Training Loss: 0.0169, Evaluation Loss 0.0167

============= Epoch 17 | 2022-09-12 15:20:51 ==============
=> Current Lr: 0.00625
[0/300]: 0.0168
[20/300]: 0.0174
[40/300]: 0.0150
[60/300]: 0.0182
[80/300]: 0.0149
[100/300]: 0.0156
[120/300]: 0.0244
[140/300]: 0.0164
[160/300]: 0.0099
[180/300]: 0.0149
[200/300]: 0.0147
[220/300]: 0.0135
[240/300]: 0.0138
[260/300]: 0.0176
[280/300]: 0.0135
=> Training Loss: 0.0168, Evaluation Loss 0.0192

============= Epoch 18 | 2022-09-12 15:22:40 ==============
=> Current Lr: 0.00625
[0/300]: 0.0179
[20/300]: 0.0229
[40/300]: 0.0159
[60/300]: 0.0181
[80/300]: 0.0152
[100/300]: 0.0144
[120/300]: 0.0151
[140/300]: 0.0191
[160/300]: 0.0254
[180/300]: 0.0326
[200/300]: 0.0173
[220/300]: 0.0178
[240/300]: 0.0128
[260/300]: 0.0151
[280/300]: 0.0177
=> Training Loss: 0.0168, Evaluation Loss 0.0151

============= Epoch 19 | 2022-09-12 15:24:30 ==============
=> Current Lr: 0.00625
[0/300]: 0.0098
[20/300]: 0.0182
[40/300]: 0.0187
[60/300]: 0.0157
[80/300]: 0.0100
[100/300]: 0.0232
[120/300]: 0.0178
[140/300]: 0.0143
[160/300]: 0.0128
[180/300]: 0.0241
[200/300]: 0.0120
[220/300]: 0.0163
[240/300]: 0.0123
[260/300]: 0.0187
[280/300]: 0.0201
=> Training Loss: 0.0165, Evaluation Loss 0.0204

============= Epoch 20 | 2022-09-12 15:26:19 ==============
=> Current Lr: 0.003125
[0/300]: 0.0247
[20/300]: 0.0136
[40/300]: 0.0116
[60/300]: 0.0146
[80/300]: 0.0091
[100/300]: 0.0129
[120/300]: 0.0149
[140/300]: 0.0128
[160/300]: 0.0171
[180/300]: 0.0135
[200/300]: 0.0109
[220/300]: 0.0159
[240/300]: 0.0101
[260/300]: 0.0135
[280/300]: 0.0146
=> Training Loss: 0.0150, Evaluation Loss 0.0139

============= Epoch 21 | 2022-09-12 15:28:07 ==============
=> Current Lr: 0.003125
[0/300]: 0.0122
[20/300]: 0.0105
[40/300]: 0.0124
[60/300]: 0.0085
[80/300]: 0.0214
[100/300]: 0.0124
[120/300]: 0.0203
[140/300]: 0.0168
[160/300]: 0.0156
[180/300]: 0.0160
[200/300]: 0.0237
[220/300]: 0.0149
[240/300]: 0.0139
[260/300]: 0.0157
[280/300]: 0.0151
=> Training Loss: 0.0147, Evaluation Loss 0.0151

============= Epoch 22 | 2022-09-12 15:29:57 ==============
=> Current Lr: 0.003125
[0/300]: 0.0149
[20/300]: 0.0188
[40/300]: 0.0154
[60/300]: 0.0166
[80/300]: 0.0186
[100/300]: 0.0114
============= Epoch 22 | 2022-09-12 21:48:02 ==============
=> Current Lr: 0.003125
[0/300]: 0.0163
[20/300]: 0.0160
[40/300]: 0.0131
[60/300]: 0.0123
[80/300]: 0.0159
[100/300]: 0.0163
[120/300]: 0.0148
[140/300]: 0.0105
[160/300]: 0.0138
[180/300]: 0.0097
[200/300]: 0.0143
[220/300]: 0.0147
[240/300]: 0.0141
[260/300]: 0.0129
[280/300]: 0.0154
=> Training Loss: 0.0146, Evaluation Loss 0.0147

============= Epoch 23 | 2022-09-12 21:49:55 ==============
=> Current Lr: 0.003125
[0/300]: 0.0144
[20/300]: 0.0149
[40/300]: 0.0179
[60/300]: 0.0168
[80/300]: 0.0099
[100/300]: 0.0130
[120/300]: 0.0108
[140/300]: 0.0129
[160/300]: 0.0134
[180/300]: 0.0149
[200/300]: 0.0095
[220/300]: 0.0142
[240/300]: 0.0128
[260/300]: 0.0147
[280/300]: 0.0113
=> Training Loss: 0.0143, Evaluation Loss 0.0139

============= Epoch 24 | 2022-09-12 21:52:07 ==============
=> Current Lr: 0.003125
[0/300]: 0.0123
[20/300]: 0.0149
[40/300]: 0.0115
[60/300]: 0.0201
[80/300]: 0.0221
[100/300]: 0.0136
[120/300]: 0.0131
[140/300]: 0.0115
[160/300]: 0.0140
[180/300]: 0.0133
[200/300]: 0.0100
[220/300]: 0.0121
[240/300]: 0.0120
[260/300]: 0.0200
[280/300]: 0.0125
=> Training Loss: 0.0140, Evaluation Loss 0.0134

============= Epoch 25 | 2022-09-12 21:54:33 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0128
[20/300]: 0.0106
============= Epoch 25 | 2022-09-12 22:04:20 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0169
[20/300]: 0.0148
[40/300]: 0.0156
[60/300]: 0.0135
[80/300]: 0.0105
[100/300]: 0.0104
[120/300]: 0.0110
[140/300]: 0.0146
[160/300]: 0.0113
[180/300]: 0.0143
[200/300]: 0.0134
[220/300]: 0.0149
[240/300]: 0.0107
[260/300]: 0.0136
[280/300]: 0.0116
=> Training Loss: 0.0128, Evaluation Loss 0.0125

============= Epoch 26 | 2022-09-12 22:06:19 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0107
[20/300]: 0.0084
[40/300]: 0.0136
[60/300]: 0.0125
[80/300]: 0.0144
[100/300]: 0.0109
[120/300]: 0.0134
[140/300]: 0.0119
[160/300]: 0.0114
[180/300]: 0.0213
[200/300]: 0.0114
[220/300]: 0.0123
[240/300]: 0.0153
[260/300]: 0.0160
[280/300]: 0.0115
=> Training Loss: 0.0127, Evaluation Loss 0.0121

============= Epoch 27 | 2022-09-12 22:10:18 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0098
[20/300]: 0.0113
[40/300]: 0.0106
[60/300]: 0.0136
[80/300]: 0.0148
[100/300]: 0.0179
============= Epoch 27 | 2022-09-12 22:20:25 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0090
[20/300]: 0.0112
[40/300]: 0.0106
[60/300]: 0.0138
[80/300]: 0.0091
[100/300]: 0.0133
[120/300]: 0.0112
[140/300]: 0.0175
[160/300]: 0.0130
[180/300]: 0.0170
[200/300]: 0.0113
[220/300]: 0.0084
[240/300]: 0.0109
[260/300]: 0.0105
[280/300]: 0.0101
=> Training Loss: 0.0125, Evaluation Loss 0.0124

============= Epoch 28 | 2022-09-12 22:22:37 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0091
[20/300]: 0.0088
[40/300]: 0.0113
[60/300]: 0.0117
[80/300]: 0.0124
[100/300]: 0.0114
[120/300]: 0.0129
[140/300]: 0.0121
[160/300]: 0.0106
[180/300]: 0.0151
[200/300]: 0.0104
[220/300]: 0.0133
[240/300]: 0.0127
[260/300]: 0.0118
[280/300]: 0.0101
=> Training Loss: 0.0122, Evaluation Loss 0.0120

============= Epoch 29 | 2022-09-12 22:25:07 ==============
=> Current Lr: 0.0015625
[0/300]: 0.0148
[20/300]: 0.0141
[40/300]: 0.0097
[60/300]: 0.0138
[80/300]: 0.0125
[100/300]: 0.0115
[120/300]: 0.0116
[140/300]: 0.0099
[160/300]: 0.0095
[180/300]: 0.0158
[200/300]: 0.0127
[220/300]: 0.0214
[240/300]: 0.0102
[260/300]: 0.0179
[280/300]: 0.0113
=> Training Loss: 0.0121, Evaluation Loss 0.0125

============= Epoch 30 | 2022-09-12 22:27:53 ==============
=> Current Lr: 0.00078125
[0/300]: 0.0076
[20/300]: 0.0086
[40/300]: 0.0065
[60/300]: 0.0131
[80/300]: 0.0150
[100/300]: 0.0148
[120/300]: 0.0106
[140/300]: 0.0134
[160/300]: 0.0109
[180/300]: 0.0162
[200/300]: 0.0123
[220/300]: 0.0102
[240/300]: 0.0090
[260/300]: 0.0086
[280/300]: 0.0105
=> Training Loss: 0.0116, Evaluation Loss 0.0109

============= Epoch 31 | 2022-09-12 22:31:02 ==============
=> Current Lr: 0.00078125
[0/300]: 0.0131
[20/300]: 0.0084
[40/300]: 0.0102
[60/300]: 0.0117
[80/300]: 0.0105
[100/300]: 0.0142
[120/300]: 0.0089
[140/300]: 0.0100
[160/300]: 0.0119
[180/300]: 0.0107
[200/300]: 0.0103
[220/300]: 0.0116
[240/300]: 0.0159
[260/300]: 0.0137
[280/300]: 0.0086
=> Training Loss: 0.0112, Evaluation Loss 0.0112

============= Epoch 32 | 2022-09-12 22:34:39 ==============
=> Current Lr: 0.00078125
[0/300]: 0.0123
[20/300]: 0.0118
[40/300]: 0.0166
[60/300]: 0.0087
[80/300]: 0.0116
[100/300]: 0.0100
[120/300]: 0.0112
[140/300]: 0.0138
[160/300]: 0.0130
[180/300]: 0.0178
[200/300]: 0.0164
[220/300]: 0.0164
[240/300]: 0.0099
[260/300]: 0.0096
[280/300]: 0.0096
=> Training Loss: 0.0113, Evaluation Loss 0.0107

============= Epoch 33 | 2022-09-12 22:38:02 ==============
=> Current Lr: 0.00078125
[0/300]: 0.0113
[20/300]: 0.0072
[40/300]: 0.0067
[60/300]: 0.0160
[80/300]: 0.0105
[100/300]: 0.0140
[120/300]: 0.0116
[140/300]: 0.0100
[160/300]: 0.0121
============= Epoch 33 | 2022-09-12 22:44:10 ==============
=> Current Lr: 0.00078125
[0/300]: 0.0147
[20/300]: 0.0067
[40/300]: 0.0123
[60/300]: 0.0133
[80/300]: 0.0106
[100/300]: 0.0093
[120/300]: 0.0090
[140/300]: 0.0116
[160/300]: 0.0102
[180/300]: 0.0183
[200/300]: 0.0093
[220/300]: 0.0123
[240/300]: 0.0129
[260/300]: 0.0143
[280/300]: 0.0116
=> Training Loss: 0.0113, Evaluation Loss 0.0111

============= Epoch 34 | 2022-09-12 22:46:11 ==============
=> Current Lr: 0.00078125
[0/300]: 0.0129
[20/300]: 0.0152
[40/300]: 0.0140
[60/300]: 0.0098
[80/300]: 0.0093
[100/300]: 0.0120
[120/300]: 0.0113
[140/300]: 0.0071
[160/300]: 0.0124
[180/300]: 0.0078
[200/300]: 0.0090
[220/300]: 0.0091
[240/300]: 0.0152
[260/300]: 0.0156
[280/300]: 0.0137
=> Training Loss: 0.0109, Evaluation Loss 0.0108

============= Epoch 35 | 2022-09-12 22:48:19 ==============
=> Current Lr: 0.000390625
[0/300]: 0.0099
[20/300]: 0.0093
[40/300]: 0.0100
[60/300]: 0.0082
[80/300]: 0.0078
[100/300]: 0.0090
[120/300]: 0.0139
[140/300]: 0.0112
[160/300]: 0.0095
[180/300]: 0.0078
[200/300]: 0.0113
[220/300]: 0.0110
[240/300]: 0.0092
[260/300]: 0.0070
[280/300]: 0.0096
=> Training Loss: 0.0104, Evaluation Loss 0.0102

============= Epoch 36 | 2022-09-12 22:50:27 ==============
=> Current Lr: 0.000390625
[0/300]: 0.0151
[20/300]: 0.0127
[40/300]: 0.0103
[60/300]: 0.0108
[80/300]: 0.0126
[100/300]: 0.0091
[120/300]: 0.0096
[140/300]: 0.0108
[160/300]: 0.0087
[180/300]: 0.0126
[200/300]: 0.0081
[220/300]: 0.0132
[240/300]: 0.0098
[260/300]: 0.0191
[280/300]: 0.0064
=> Training Loss: 0.0103, Evaluation Loss 0.0102

============= Epoch 37 | 2022-09-12 22:52:36 ==============
=> Current Lr: 0.000390625
[0/300]: 0.0121
[20/300]: 0.0129
[40/300]: 0.0104
[60/300]: 0.0143
[80/300]: 0.0171
[100/300]: 0.0104
[120/300]: 0.0084
[140/300]: 0.0118
[160/300]: 0.0097
[180/300]: 0.0105
[200/300]: 0.0081
[220/300]: 0.0100
[240/300]: 0.0114
[260/300]: 0.0101
[280/300]: 0.0134
=> Training Loss: 0.0102, Evaluation Loss 0.0103

============= Epoch 38 | 2022-09-12 22:57:33 ==============
=> Current Lr: 0.000390625
[0/300]: 0.0103
[20/300]: 0.0067
[40/300]: 0.0072
[60/300]: 0.0140
[80/300]: 0.0108
[100/300]: 0.0170
[120/300]: 0.0149
[140/300]: 0.0103
[160/300]: 0.0119
[180/300]: 0.0098
[200/300]: 0.0072
[220/300]: 0.0122
[240/300]: 0.0107
[260/300]: 0.0062
[280/300]: 0.0070
=> Training Loss: 0.0101, Evaluation Loss 0.0103

============= Epoch 39 | 2022-09-12 23:02:17 ==============
=> Current Lr: 0.000390625
[0/300]: 0.0159